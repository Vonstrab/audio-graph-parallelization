%% LyX 2.3.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\RequirePackage{fixltx2e}
\documentclass[french]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,lmargin=1.5cm,rmargin=1.5cm}
\usepackage{babel}
\makeatletter
\addto\extrasfrench{%
   \providecommand{\og}{\leavevmode\flqq~}%
   \providecommand{\fg}{\ifdim\lastskip>\z@\unskip\fi~\frqq}%
}

\makeatother
\usepackage{varioref}
\usepackage{float}
\usepackage{algorithm2e}
\usepackage{graphicx}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\LinesNumbered
\usepackage{adjustbox}

\@ifundefined{showcaptionsetup}{}{%
 \PassOptionsToPackage{caption=false}{subfig}}
\usepackage{subfig}
\makeatother

\begin{document}
\title{Rapport du PSAR/PSTL: Parallélisation de graphes audio (sujet PSTL)}
\author{Eisha Chen-yen-su, Ivan Delgado}
\maketitle

\part*{Introduction}

Le présent document porte sur un projet STL, il a donc été réalisé
dans le cadre de ce dernier mais également dans celui du projet SAR
car l\textquoteright un des membres du binôme est en SAR. Il comportera
donc des éléments provenant du cahier des charges qui a été rendu
pour le PSAR, mais comme ce dernier n\textquoteright était pas exigé
pour le PSTL, il nous a semblé judicieux de faire cela pour qu\textquoteright il
ne manque aucun éléments de contexte.

~

Dans ce rapport, nous aborderons en premier lieu les graphes audio,
ainsi que ce qui les caractérisent. Nous présenterons ensuite les
algorithmes d\textquoteright ordonnancement ainsi que l\textquoteright architecture
logicielle ayant permis leur mise en ½uvre. Nous ferons également
une comparaison entre les performances de ces ordonnancements. Finalement,
nous ferons une critique de la réalisation du projet en indiquant
les aspects de l\textquoteright implémentation qui pourraient être
améliorés et les fonctionnalités qui pourraient être développées par
la suite.

\subsection*{Présentation des besoins}

Les systèmes interactifs musicaux (abrégé SIM) permettent de jouer
et de composer de la musique en temps réel. Ces derniers permettent
la synthèse de signaux audio, qui est le résultat d\textquoteright un
ensemble de traitements sur ces signaux. Ces traitements doivent se
faire en temps réel : à chaque cycle audio, tous les calculs doivent
se faire dans un temps imparti (i.e. avant la fin du cycle), sans
quoi, il y a une dégradation de la qualité du signal audio. Or la
complexification de ces traitements font que cette synthèse peut nécessiter
de plus en plus de calculs mais tout en devant être sur les mêmes
durées, il devient donc difficile de ne pas avoir de dégradations
dans ces conditions.

Une solution consiste à tirer profit de l\textquoteright architecture
multi-c½ur des processeurs modernes pour paralléliser ces calculs.

~

Nous allons préciser ici les contraintes temps réel que doivent respecter
les traitements audio en nous inspirant de l\textquoteright analyse
réalisée dans \cite{key-1}.

Un signal audio est caractérisé par sa fréquence d\textquoteright échantillonnage
indiquant le nombre de valeurs (chaque valeur étant un nombre à virgule
flottante ou un entier) qu\textquoteright il doit prendre à chaque
secondes, le plus souvent elle est de 44,1 kHz. La carte son d\textquoteright un
ordinateur va lire ces échantillons dans un buffer audio de taille
fixe à un intervalle régulier (c\textquoteright est ce qui défini
un cycle audio), donc si nous avons un buffer de 512 échantillons,
la carte va lire dans ce buffer environ 86 fois par seconde (44 100
divisé par 512), donc toutes les 11,6 millisecondes\footnote{L'API JACK que nous utilisons utilise deux buffers, le principe est
le même, la durée des cycle est doublée}. Si un nouveau buffer n\textquoteright est pas disponible au delà
de cette échéance, alors la carte audio va se mettre à lire des échantillons
nuls ou bien le même buffer qu\textquoteright au cycle précédent,
ce qui produit un \textquotedblleft tick\textquotedblright{} désagréable
à l\textquoteright oreille. La durée d\textquoteright un cycle audio
constitue donc la contrainte temps réel que doit respecter le traitement
audio : le temps de calcul des traitements audio, pour chaque buffer,
doit impérativement être inférieur à la durée d\textquoteright un
cycle.

~

Ces traitements audio peuvent êtres représentés par un graphe audio
(ou DAG, pour \textquotedblleft Directed Acyclic Graph\textquotedblright ).
Les n½uds d\textquoteright entrées d\textquoteright un tel graphe
sont les sources des signaux audio et les n½uds de sorties, les sorties
audio du système. Les autres n½uds du graphe sont des traitements
altérant les signaux audio passant par ces derniers. Ainsi chaque
signal va-t-il suivre un chemin audio en passant d\textquoteright un
n½ud à un autre jusqu\textquoteright à arriver à l\textquoteright une
des sorties.

~

Un graphe audio peut être vu comme un graphe de tâches dont chaque
n½ud représente un traitement audio (en anglais, \textquotedblleft DSP\textquotedblright{}
pour \textquotedblleft Digital Signal Processor\textquotedblright )
et les arcs représentent les buffers ou canaux permettant à deux DSP
de communiquer entre eux. De plus, il est évident qu\textquoteright un
arc représentant une communication d\textquoteright un n½ud A vers
B induit que la tâche B dépend de la tâche A qui doit s\textquoteright exécuter
avant.

Le poids d\textquoteright un n½ud correspond au temps nécessaire à
l\textquoteright exécution d\textquoteright une tâche. Celui d\textquoteright un
arc correspond au coût de communication entre deux tâches. Le coût
de communication peut être considéré comme négligeable dans le cas
d\textquoteright une communication via une mémoire partagée, elle
peut être beaucoup plus importante et variable dans le cas de passages
de messages, il faut alors en tenir compte.

\begin{figure}[H]

\begin{centering}
\includegraphics[scale=0.25]{dot/seq_test/seq_test-1}\caption{Exemple de DAG}
\par\end{centering}
\end{figure}

~

Le chemin critique d\textquoteright un graphe de tâches correspond
au plus long chemin (par rapport à la somme totale des poids des n½uds
et des arcs le constituant) entre un n½ud d\textquoteright entrée
et un n½ud de sortie du DAG. Et la longueur de ce chemin est le temps
minimum (possible) d\textquoteright exécution parallèle de ce graphe.

Donc paralléliser un graphe de tâches revient à faire un ordonnancement
périodique de ses tâches entre les différents processeurs disponibles,
tout en respectant les dépendances entre ces dernières.

~

La parallélisation des graphes audio est déjà quelque chose de connue,
cependant elle n\textquoteright est pas supportée par tous les SIM
ou alors elle exige l\textquoteright utilisation d\textquoteright instructions
explicites. Notre objectif était donc d\textquoteright étudier les
performances d\textquoteright algorithmes d\textquoteright ordonnancement
de graphes de tâches vis-à-vis des contraintes temps réel du domaine
audio, pour permettre la parallélisation automatique de l\textquoteright exécution
de graphes audio.

\part*{Algorithmes utilisés}

Nous allons à présent parler des différents algorithmes et stratégies
utilisées pour étudier les possibilités de parallélisation automatique
de graphes audio. Nous avons vu qu\textquoteright il s\textquoteright agissait
d\textquoteright un problème d\textquoteright ordonnancement de graphes
de tâches.

~

\subsection*{Taxonomie de l\textquoteright ordonnancement}

Tout d\textquoteright abord, il y a deux types d\textquoteright ordonnancements
de DAG : l\textquoteright ordonnancement statique, qui définit quelles
tâches vont s\textquoteright exécuter sur quels processeurs à un instant
donné, avant son exécution (comme au moment de la compilation par
exemple) et qui sera toujours le même; l\textquoteright ordonnancement
dynamique qui quand à lui est déterminé au fur et à mesure de l\textquoteright exécution
du graphe et qui par conséquent, peut varier en fonction d\textquoteright éventuels
aléas des différentes exécutions.

Les algorithmes d\textquoteright ordonnancement statique sont tirés
de \cite{key-1} et la stratégie d\textquoteright ordonnancement dynamique
est tirée de \cite{key-2} et \cite{key-3}.

~

Nous pouvons dégager certaines caractéristiques des DAG dans les cas
étudiés : il n\textquoteright y a pas de tâches parallèles (i.e. une
tâche n\textquoteright est exécutée que sur un seul processeur), le
coût de calcul des tâches est arbitraire et les coûts de communication
entre les tâches seront considérés comme négligeables.

Ces DAG seront exécutés sur des systèmes multiprocesseurs à mémoire
partagée et les processeurs ont tous la même vitesse de traitement.

\section*{Les algorithmes d\textquoteright ordonnancement statique}

\subsection*{Le principe de l\textquoteright ordonnancement à liste}

Nous allons définir le principe général derrière les algorithmes d\textquoteright ordonnancement
statiques.

~

Ces algorithmes construisent d\textquoteright abord une liste des
tâches ordonnées par ordre décroissant de priorité. À chaque étape,
il retire la tâche la plus prioritaire de la liste, puis l\textquoteright ordonnance
sur le processeur permettant à la tâche de commencer le plus tôt possible.
Il se termine lorsque toutes les tâches ont été assignées à l\textquoteright un
des processeurs. Certains algorithmes calculent les priorités des
tâches une seule fois au début, d\textquoteright autres les évaluent
à chaque itération.

Deux attributs sont fréquemment utilisés pour déterminer la priorité
d\textquoteright une tâche : le t-level (\textquotedblleft top level\textquotedblright )
et le b-level (\textquotedblleft bottom level\textquotedblright ).
Le t-level d\textquoteright un n½ud n correspond à la longueur (i.e.
la somme des poids des n½uds et des arcs) maximale d\textquoteright un
chemin allant de l\textquoteright un des n½uds d\textquoteright entrée
du DAG vers n (en excluant son propre poids). Le b-level d\textquoteright un
n½ud n est quand à lui la longueur maximale d\textquoteright un chemin
allant de n vers un n½ud de sortie du graphe. Il y a aussi une variante
statique du b-level pour laquelle nous faisons seulement la somme
des poids des n½uds : le \textquotedblleft static level\textquotedblright{}
(que nous abrégeons \textquotedblleft SL\textquotedblright ).

\subsection*{L\textquoteright algorithme HLFET avec départage CP/MISF}

Le premier algorithme est l\textquoteright algorithme HLFET (pour
\og Highest Level First with Estimated Times\fg ) avec un départage
des égalités avec CP/MISF (pour \og Critical Path / Most Immediate
Successors First\fg ). Pour celui-ci, la priorité d\textquoteright un
n½ud est définie par son SL. Lorsque plusieurs n½uds ont le même SL,
on choisi d\textquoteright abord celui ayant le plus grand nombre
de successeurs.

Lorsque l\textquoteright on considère négligeable les coûts de communication
entre les tâches, cet algorithme est supposé être proche de l\textquoteright ordonnancement
optimal, comme il privilégie les n½uds appartenant au chemin critique.
De plus, il est très simple à mettre en place. C\textquoteright est
pour ces raisons que nous avons choisi cet algorithme en premier.

\subsection*{L\textquoteright algorithme ETF}

Le second algorithme statique est l\textquoteright algorithme ETF
(pour \textquotedblleft Earliest Time First\textquotedblright ). À
chaque itération, il faut calculer pour chaque n½ud prêt (i.e. dont
tous les parents ont déjà été ordonnancés), et sur chaque processeur,
l\textquoteright instant le plus tôt auquel il pourra s\textquoteright exécuter.
Puis on ordonnance le n½ud pouvant s\textquoteright exécuter au plus
tôt sur le processeur le permettant. Les égalités sont résolues en
choisissant d\textquoteright abord le n½ud ayant le SL le plus élevé.

Cet algorithme est également simple à implémenter. De plus, il privilégie
au mieux les temps de démarrage les plus précoces ainsi que les n½uds
du chemin critique. Il est aussi possible de borner la qualité du
résultat par rapport à l\textquoteright ordonnancement optimal.

\subsection*{L\textquoteright algorithme CPFD}

Le dernier algorithme statique est l\textquoteright algorithme CPFD
(pour \textquotedblleft Critical Path Fast Duplication\textquotedblright ).
Il est différent des autres vus précédemment, car il va ordonnancer
les tâches via un procédé plus complexe qu\textquoteright avec une
liste ordonnée. CPFD s\textquoteright autorise à dupliquer des tâches
lorsque cela permet d\textquoteright éviter les coûts de communication
d\textquoteright une tâche A vers une tâche B : si une tâche B s\textquoteright exécute
juste après A sur le même processeur, alors le résultat de A est déjà
disponible (sur ce processeur) pour que B puisse s\textquoteright exécuter
immédiatement après. Il n\textquoteright y a donc pas besoin d\textquoteright envoyer
de messages pour communiquer le résultat de A à B (ce qui peut être
significativement long). Cela peut être particulièrement utile dans
un système réparti : faire en sorte que deux tâches directement dépendantes
s\textquoteright exécutent sur un même site permet d'éviter des envoies
de messages et peut donc faire gagner du temps.

Dans le cadre du domaine audio, cela peut être mis en application
lors d\textquoteright une collaboration en temps réel entre plusieurs
musiciens étant sur plusieurs machines différentes ou éloignés géographiquement.

CPFD permet donc de prioriser les n½uds du chemin critique et peut
faire plus de réductions de coûts de communication via la duplication
de tâche, il semble donc bien adapté pour s\textquoteright assurer
de respecter au mieux une contrainte temps réel sur un système réparti.

Ce qui va suivre est la description de l\textquoteright algorithme.

~

Premièrement, CPFD va distinguer les n½uds du DAG en trois catégories
: les CPN (pour \textquotedblleft Critical Path Node\textquotedblright ),
qui sont les n½uds appartenant à un chemin critique; les IBN (pour
\textquotedblleft In-Branch Nodes\textquotedblright ), qui sont les
n½uds possédant un chemin menant à un CPN; et les OBN (pour \textquotedblleft Out-Branch
Node\textquotedblright ), qui sont simplement les n½uds n\textquoteright appartenant
pas aux deux autres catégories. CPFD s\textquoteright appuie sur une
liste appelée \textquotedblleft CPN-Dominant Sequence\textquotedblright .
Elle est construite selon l\textquoteright algorithme \vref{Algorithme 1}
:

~

\begin{algorithm}[h]
\label{Algorithme 1}\caption{Sequencement des CPN}

Insérer en premier le CPN d\textquoteright entrée du DAG dans la séquence,
mettre \emph{Position} à 2. Soit nx, le n½ud suivant dans le chemin
critique

\textbf{Répéter} :

\Indp

\textbf{Si} nx a tous ses parents dans la séquence

\Indp

mettre nx à \emph{Position} dans la séquence et incrémenter \emph{Position}.

\Indm

\textbf{Sinon}

\Indp

Posons ny tel qu\textquoteright il soit un éventuel parent de nx qui
ne soit pas dans la séquence et avec le \emph{b-level} le plus élevé
(si plusieurs n½uds correspondent à cela, on choisi d\textquoteright abord
celui ayant le t-level minimal).

\textbf{Si} ny a tous ses parents dans la séquence

\Indp

alors l\textquoteright insérer à \emph{Position} et incrémenter \emph{Position}.

\Indm

\textbf{Sinon}

\Indp

inclure récursivement tous les prédécesseurs de ny dans la séquence.

Répéter l\textquoteright étape précédente jusqu\textquoteright à ce
que tous les parents de nx soient dans la séquence. Insérer nx dans
la séquence à \emph{Position}.

\Indm

\textbf{Fin si}.

\Indm

\textbf{Fin si}.

\Indm

\textbf{Jusqu\textquoteright à ce que} tous les CPN soient dans la
séquence.

Ajouter à la fin de la séquence, tous les OBN par ordre décroissant
de \emph{b-level}.
\end{algorithm}

Premièrement, CPFD va distinguer les n½uds du DAG en trois catégories~:
\begin{itemize}
\item Les CPN (pour \textquotedblleft Critical Path Node\textquotedblright ),
qui sont les n½uds appartenant à un chemin critique.
\item Les IBN (pour \textquotedblleft In-Branch Nodes\textquotedblright ),
qui sont les n½uds possédant un chemin menant à un CPN.
\item Les OBN (pour \textquotedblleft Out-Branch Node\textquotedblright ),
qui sont simplement les n½uds n\textquoteright appartenant pas aux
deux autres catégories.
\end{itemize}
À partir de cette séquence, CPFD procède à l\textquoteright ordonnancement
selons l'algorithme \vref{Algorithme 2}:

\begin{algorithm}[ph]
\label{Algorithme 2}

\caption{Ordonnancement CPFD}

Soit \emph{candidate} le CPN d\textquoteright entrée.

\textbf{Répéter:}

\Indp

Soit \emph{P\_SET}, l\textquoteright ensemble des processeurs comportant
les parents de \emph{candidate}, plus un processeur inutilisé.

\textbf{Pour chaque} \emph{P} dans \emph{P\_SET}, faire :

\Indp

\textbf{(a) }Calculer \emph{ST} : le temps de départ de \emph{candidate}
sur \emph{P}.

\textbf{(b) }Soit m, un éventuel parent de \emph{candidate} qui n\textquoteright est
pas ordonnancé sur \emph{P} et dont le message pour \emph{candidate
}a le temps d\textquoteright arrivée le plus tardif.

\textbf{(c)} Essayer de dupliquer m sur le créneau d\textquoteright inactivité
de \emph{P} le plus précoce.

\Indp

\textbf{Si} la duplication réussi et que cela diminue \emph{ST}

\Indp

alors mettre à jour \emph{ST}. Faire en sorte qu\textquoteright un
nouveau candidate soit m et retourner à l\textquoteright étape \textbf{a}.
.

\Indm

\textbf{Sinon}

\Indp

si la duplication échoue, alors rendre le contrôle pour examiner un
autre parent du candidate précédent.

Ordonnancer candidate sur le processeur \emph{P\textquoteright{}}
qui lui permet de commencer le plus tôt et faire les duplications
requises.

Soit \emph{candidate}, le CPN suivant.

Répéter le processus de l\textquoteright étape \textbf{3}. à \textbf{6}.
pour chaque OBN avec \emph{P\_SET} contenant tous les processeurs
utilisés, plus un processeur inutilisé. Les OBN sont ordonnancés dans
l\textquoteright ordre topologique.

\Indm

\textbf{Fin si}

\Indm

\textbf{Jusqu\textquoteright à ce que} tous les CPN soient ordonnancés.
\end{algorithm}

La toute dernière stratégie d\textquoteright ordonnancement étudiée
est l\textquoteright ordonnancement avec vole de tâches. Il s\textquoteright agit
d\textquoteright un ordonnancement dynamique dans lequel les tâches
sont exécutées par un pool de threads.

Chaque thread a une file d\textquoteright attente. Au début de l\textquoteright exécution
du DAG, les tâches prêtes (i.e. celles correspondant aux n½uds d\textquoteright entrée
du DAG) sont réparties parmi les files d\textquoteright attentes de
chacun des threads en étant insérées au début de ces files. Pour avoir
la prochaine tâche à exécuter, un thread en prend une au début de
sa file (ordre LIFO). Si sa file est vide, il va \textquotedblleft voler\textquotedblright{}
une tâche à la fin (ordre FIFO) de la file d\textquoteright attente
d\textquoteright un autre thread.

À chaque fois qu\textquoteright un thread fini une tâche, il va ajouter,
au début de sa file, la (ou les) tâche(s) nouvellement prête(s) (i.e.
qui en dépendait et dont leurs autres dépendances ont également été
satisfaites).

Cette stratégie a pour premier avantage d\textquoteright utiliser
des files d\textquoteright attentes qui peuvent êtres \textquotedblleft lock-free\textquotedblright ,
donc il y a peu de contentions sur ces dernières et il n\textquoteright y
a pas de surcoûts causés par des synchronisations. Comme un thread
va toujours tenter de suivre un chemin de calcul du DAG (car il va
essayer d\textquoteright exécuter immédiatement, après une tâche,
ses successeurs), il y a augmentation de la localité des données et
on a alors une plus grande probabilité pour que les données, sur lesquelles
travaille un thread, restent dans les caches du processeur. De plus,
lorsqu\textquoteright un thread vole une tâche dans une file F, cette
tâche sera celle avec le \emph{t-level} le plus petit de la file F,
donc avec une priorité plus élevée que les autres.

\part*{Architecture logicielle}

Dans cette partie, nous ne parlerons pas de façon détaillée de l\textquoteright implémentation
: en effet, cette dernière est déjà amplement commentée et documentée
dans le code source du projet. De même, la documentation utilisateur
est contenue dans le README du projet.

Nous nous concentrerons ici sur la présentation de l\textquoteright architecture
globale ainsi que sur les points importants de l\textquoteright implémentation.

\subsubsection*{Les fonctionnalités réalisées}

Nous allons présenter les fonctionnalités réalisées par le logiciel
développé durant le projet. Le logiciel peut lire certains graphes
audio écrits dans des fichiers au format AudioGraph. Il peut créer
des fichiers DOT et PDF (avec \textbf{Graphviz} \cite{key-4}) représentant
le graphe. Il peut calculer des ordonnancements statiques pour ces
graphes avec les algorithmes \emph{HLFET}, \emph{ETF}, \emph{CPFD
}ou \textquotedblleft random\textquotedblright{} (on assigne des priorités
aléatoires aux n½ud lors de l\textquoteright ordonnancement) et éventuellement
les afficher. Il peut exécuter séquentiellement le graphe audio ou
en parallèle avec des threads \textbf{POSIX} selon un ordonnancement
statique ou un ordonnancement dynamique avec vol de tâches. Pour finir,
il est également possible de faire des mesures sur les temps des exécutions
d'un graphe audio.

\subsubsection*{Les technologies utilisées}

Nous allons à présent parler des technologies qui sont utilisées par
ce projet.

Le logiciel est écrit en \textbf{Rust }\cite{key-5}. C\textquoteright est
un langage de programmation dont le compilateur produit du code rapide
(le compilateur utilisant \textbf{LLVM }\cite{key-6}comme backend),
il est orienté vers la programmation concurrente et le compilateur
fait de nombreuses vérifications pour détecter des problèmes liés
à la mémoire (comme par exemple le déréférencement de pointeurs non
valides ou les data races).

~

\textbf{Jack }\cite{key-7} est utilisé pour la fonction de callback
audio. \textbf{JACK} fait référence à une API et aussi à l\textquoteright implémentation
d\textquoteright une infrastructure permettant à des applications
audio de communiquer entre elles et avec les interfaces audio (comme
des cartes son). À chaque cycle audio, le serveur audio \textbf{JACK}
va appeler la fonction de callback audio de notre application. C\textquoteright est
dans cette fonction que va être appelée notre routine d\textquoteright exécution
du graphe audio. Les n½uds de sortie du graphe vont écrire les résultats
des traitements du graph dans les buffers des ports de sortie de l\textquoteright application,
ce sont ces buffers qui vont être lus par le serveur \textbf{JACK}.

~

\textbf{crossbeam }\cite{key-12} est utilisé pour diverses mécanismes
de synchronisation tels que~:
\begin{itemize}
\item Les \emph{channel} qui sont des canaux de messages multi-producteurs
et multi-consommateurs.
\item Les \emph{deque} qui sont une implémentation de files utilisables
pour l\textquoteright ordonnancement par vole de tâche.
\item Les \emph{ShardedLock} qui sont des verrous permettant à une ressource
d\textquoteright être verrouillée pour un seul écrivain ou pour plusieurs
lecteurs. Ce type de verrous existent déjà dans la bibliothèque standard
(\emph{RwLock}) mais ceux de \textbf{crossbeam }sont plus rapide pour
acquérir le verrou en lecture mais plus lent pour l\textquoteright acquérir
en écriture, ce qui est mieux lorsque l\textquoteright on sait qu\textquoteright on
va plus souvent lire une donnée que la modifier.
\item L\textquoteright utilitaire \emph{Backoff} permet de faire des boucles
d\textquoteright attentes actives mais en réduisant la contention
sur le processeur, en faisant en sorte que le thread rende la main
à l\textquoteright OS, au bout d\textquoteright un certain temps,
pour des durées qui croissent exponentiellement à chaque fois.
\end{itemize}
~

\textbf{core\_affinity} \cite{key-13} permet de faire en sorte qu\textquoteright un
thread s\textquoteright exécute toujours le même processeur. Ceci
est extrêmement important pour la performance de l\textquoteright exécution
des DAG, car les algorithmes d\textquoteright ordonnancement s'appuient
sur la localité des données : lorsque deux tâches s\textquoteright exécutent
successivement et que l\textquoteright une utilise le résultat de
l\textquoteright autre (i.e. il y a une relation de dépendance entre
ces tâches dans le DAG), il est beaucoup plus avantageux qu\textquoteright elles
s\textquoteright exécutent sur le même c½ur car la seconde tâche peut
plus rapidement accéder aux données qui ont été écrites par la première
et qui sont encore dans le cache du c½ur, au lieu d\textquoteright avoir
à les chercher dans la RAM, ce qui est systématiquement le cas si
les tâches s\textquoteright exécutent sur des c½urs différents.

~

\textbf{criterion} \cite{key-14} permet de faire des benchmarks pour
estimer les WCET (Worst Case Execution Time) des n½uds du DAG, qui
seront utilisés comme coût de calcul des n½uds par les algorithmes
d\textquoteright ordonnancement statiques.

~

\textbf{pest }\cite{key-15} est utilisé pour écrire le parser de
fichiers \textbf{AudioGraph}.

\subsubsection*{Présentation des modules Rust}

\textbf{Rust} permet de simplement structurer le code d\textquoteright un
logiciel en modules.

Voici la liste des modules du projet :
\begin{itemize}
\item \emph{dsp} : contient l\textquoteright implémentation des traitements
audio ainsi que des buffers permettant de communiquer entre les tâches.
\item \emph{execution} : implémente les exécutions des DAG. Il y a l\textquoteright exécution
séquentielle mais aussi l\textquoteright exécution parallèle des ordonnancements
statiques et l\textquoteright exécution parallèle avec ordonnancement
par vol de tâches.
\item \emph{measure} : contient les fonctions permettant d\textquoteright effectuer
les mesures temporelles sur les différentes exécutions.
\item \emph{parser} : contient le parser pour extraire les graphes audio
décrits dans des fichiers AudioGraph.
\item \emph{static\_scheduling} : contient l\textquoteright implémentation
des algorithmes d\textquoteright ordonnancement statique.
\item \emph{task\_graph} : implémente la représentation d\textquoteright un
DAG avec diverses informations utilisées par les algorithmes d\textquoteright ordonnancement
statique ou l\textquoteright exécution parallèle avec ordonnancement
par vol de tâches.
\end{itemize}

\subsubsection*{Les DSP et fichiers AudioGraph supportés}

Nous allons détailler ici la manière dont a été implémenté le traitement
du signal. Nous nous sommes efforcé de garder un séparation entre
la représentation du DAG contenant les informations utiles à l\textquoteright ordonnancement
(le graphe de tâches) et le graphe audio sous-jacent où est effectué
le traitement du signal.

Les n½uds de ce graphe audio sont des fonctions prenant un (ou plusieurs)
signal en entrée, sous forme d\textquoteright un (ou plusieurs) tableau
de flottants (qui sont les échantillons audio), et écrivant le résultat
du traitement dans un (ou plusieurs) autre tableau de flottants. Les
n½uds d\textquoteright entrée du graphe ne prennent pas de signal
en entrée mais en fournissent un en sortie, les n½uds de sortie ont
seulement une entrée.

Les arcs du graphes sont les buffers reliant les traitements : une
fonction de traitement écrit son résultat dans ce buffer (c\textquoteright est
sa sortie) et une autre lit dans ce même buffer (c\textquoteright est
son entrée). Ces buffers viennent de la structure \emph{DspEdge} qui
contient toutes les caractéristiques du signal : les échantillons
du signal, la fréquence d\textquoteright échantillonnage et la taille
du buffer.

~

Les DSP implémentés sont les suivants :
\begin{itemize}
\item \emph{Oscillator} : un oscillateur produisant une onde sinusoïdale
avec une certaine amplitude et fréquence. Il n\textquoteright a qu\textquoteright une
seule sortie.
\item \emph{Modulator }: applique une modulation au signal d\textquoteright entrée
qui peut s\textquoteright apparenter à de la modulation d\textquoteright amplitude
(AM). Il a exactement une entrée et une sortie.
\item \emph{Sink} : ce DSP est la sortie du graphe audio, il écrit le signal
d\textquoteright entrée dans un buffer audio de \textbf{JACK}. Il
n\textquoteright a qu\textquoteright une seule entrée.
\item \emph{InputOutputAdaptator }: il s\textquoteright agit d\textquoteright un
mixeur, il peut avoir plusieurs entrées et sorties. Il mixe les signaux
en entrée et écrit le même résultat tous ses buffers de sortie.
\end{itemize}
Lorsque le programme lit un fichier AudioGraph, il va reconnaître
les n½uds déclarés comme les DSP implémentés. Si le parser trouve
un n½ud dont le type est inconnu, il va lui assigner un DSP par défaut
en fonction du nombre de ses entrées et sorties.

\subsubsection*{Implémentation de l\textquoteright exécution séquentielle}

L\textquoteright exécution séquentielle du graphe audio est réalisée
dans la fonction\emph{ run\_seq}. Cette fonction va initialement faire
une liste des tâches en effectuant un tri topologique du DAG. Puis,
à chaque cycle audio, va exécuter le DAG en exécutant séquentiellement
les tâches dans l\textquoteright ordre de cette liste.

\subsubsection*{Implémentation des algorithmes d\textquoteright ordonnancement statique}

Les ordonnancements statiques ont été implémentés avec des fonctions
prenant en paramètre un graphe de tâche (la structure \emph{TaskGraph})
et retournant un ordonnancement. L\textquoteright ordonnancement est
représenté dans la structure Schedule qui contient la liste des processeurs
participant à l\textquoteright exécution des tâches (la structure
\emph{Processor}). Chaque processeur a une liste de fenêtres temporelles
(la structure \emph{TimeSlot}) indiquant quand il doit commencer une
tâche et quand il doit la finir (qui est simplement la date de début
plus le WCET), ces fenêtres ne se chevauchent évidemment pas, car
un même processeur ne peut qu\textquoteright exécuter séquentiellement
les tâches.

Il y a quatres fonctions pour ces ordonnancements : \emph{random}
qui réalise une stratégie d\textquoteright ordonnancement où l\textquoteright on
donne une priorité aléatoire aux différents n½uds, et \emph{hlfet},
\emph{etf} et \emph{cpfd} qui réalisent les algorithmes du même nom.

~

L\textquoteright exécution des ordonnancements statiques est réalisée
dans la fonction\emph{}\\
\emph{run\_static\_sched}. Cette fonction va initialement calculer
l\textquoteright ordonnancement statique sur n processeurs (attention,
ce nombre ne soit pas dépasser le nombre de c½urs disponibles sur
la machine) du DAG en fonction d\textquoteright un des algorithmes
implémentés, puis crée un pool de n thread qui a connaissance de l\textquoteright ordonnancement.
Ainsi chaque thread se retrouve affecté à l\textquoteright exécution
de l\textquoteright un des \textquotedblleft processeurs\textquotedblright{}
décrit dans l\textquoteright ordonnancement statique et chaque thread
va toujours s\textquoteright exécuter sur un même c½ur, grâce à \textbf{core\_affinity}.
On a alors une bonne correspondance entre ce qui est décrit dans les
ordonnancements statique et ce qui sera réellement effectué lors des
exécutions du DAG.

Chaque thread a donc une liste de tâche (celle de la structure Processor
qui leur a été attribuée) et va l\textquoteright exécuter une à une
en respectant l\textquoteright ordre. Cependant, cette implémentation
ne tient pas compte des fenêtres temporelles de la liste de tâches
mais va essayer d\textquoteright exécuter les tâches dès que possible
(car le plus souvent, le calcul d\textquoteright une tâche prendra
moins de temps que le WCET), mais il y a un mécanisme de synchronisation
pour que les dépendances entre les tâches soient quand même respectées.

Chaque tâche est associée à un état (l\textquoteright énumération
\emph{TaskState}) indiquant si elle est prête à être exécutée (i.e.
si toutes les tâches parentes ont fini de s\textquoteright exécuter)
ou non. Dans le cas où elle ne l\textquoteright est pas, il y a un
compteur indiquant combien de parents doivent encore s\textquoteright exécuter
(le compteur d\textquoteright activation).

~

Quand un thread finit d\textquoteright exécuter une tâche, il change
son état pour indiquer qu\textquoteright elle est finie et va décrémenter
les compteurs d\textquoteright activation des tâches dépendantes et
si le compteur d\textquoteright activation d\textquoteright une tâche
atteint 0, elle devient prête.

~

Quand un thread veut commencer à exécuter une tâche, il doit d\textquoteright abord
vérifier si celle-ci est prête. Si ce n\textquoteright est pas le
cas, il entamer une attente active avec l\textquoteright utilitaire
\emph{Backoff}. L\textquoteright attente active permet d\textquoteright éviter
que les threads ne s\textquoteright endorment systématiquement pour
attendre qu\textquoteright une tâche soit prête. Ce genre de mécanisme
de synchronisation engendre des appels systèmes, ce qui d\textquoteright une
part utilise beaucoup de cycles CPU et d\textquoteright une autre,
comme les threads doivent rendre la main au noyau, lorsqu\textquoteright ils
reviennent de l\textquoteright appel système, les données qui leur
sont utiles ont probablement été évincées des caches du processeur.
Pour ces raisons, il faut éviter le plus possible de faire des appels
systèmes lorsqu\textquoteright on programme une application temps
réel.

\subsubsection*{Implémentation de l\textquoteright exécution de l\textquoteright ordonnancement
dynamique avec vol de tâches}

L\textquoteright exécution parallèle avec l\textquoteright ordonnancement
par vol de tâches est réalisée dans la fonction run\_work\_stealing.
La fonction va initialement créer un pool de n threads. Chaque thread
a sa propre file de tâches (de type Worker, venant de crossbeam) et
il y a une file globale (de type Injector). Les threads vont initialement
chercher les tâches dans la file globale. Puis, à chaque fin de tâche,
ils vont décrémenter les compteurs d\textquoteright activations des
tâches dépendantes (cela marche comme pour l\textquoteright exécution
des ordonnancements statiques), ajouter au début de leur file les
nouvelles tâches prêtes et finalement prendre la prochaine tâche,
au début de leur file ou en volant à la fin d\textquoteright une des
autres files.

\subsubsection*{Mécanisme de contrôle des pools de threads}

Pour les deux exécutions parallèles, lorsque le programme doit commencer
l\textquoteright exécution d\textquoteright un graphe, il appel une
méthode du thread de pool qui envoie un message à chaque thread pour
leur indiquer de commencer les calculs. Chaque thread dispose d\textquoteright un
canal de messages sur lequel il attend un message avant de commencer
à exécuter le graphe. À chaque fois qu\textquoteright un thread fini
toutes les tâches à exécuter, il va attendre sur son canal le prochain
message lui indiquant que le graphe doit à nouveau être exécuté.

\subsubsection*{Réalisation des mesures de performance}

Nous voulions faire des mesures sur le temps d\textquoteright exécution
total d\textquoteright un graphe à chaque cycle audio. Pour cela,
au début de la fonction de callback audio, la date est enregistrée.
Il y a un second enregistrement de la date juste après la fin de l\textquoteright exécution
du graphe audio. Au lancement du programme, il y a un thread qui est
créé et qui est associé à un canal de messages. Ce thread va simplement
attendre de recevoir des messages sur ce canal et va enregistrer dans
un fichier le contenu de chaque message reçu. À chaque fois que le
thread principal fait un enregistrement, il envoie un message contenant
l\textquoteright information enregistrée au thread qui va écrire ces
informations dans un fichier. L\textquoteright intérêt d\textquoteright utiliser
un thread séparé pour écrire les mesures dans un fichier est d\textquoteright éviter
aux autres threads d\textquoteright avoir à effectuer des appels systèmes
(ce qui doit être fait si l\textquoteright on veut manipuler des fichiers),
ils n\textquoteright ont qu\textquoteright à envoyer un message dans
le canal vers le thread écrivain et cette opération est toujours non-bloquante
pour les canaux de type unbounded (fournis par crossbeam). Les mesures
ont ainsi un impact limité sur l\textquoteright exécution des graphes
audio.

~

Un script Python a été écrit pour permettre de facilement générer
des données statistiques permettant de faire des comparaisons entre
les différents algorithmes de parallélisation. Le script va lancer
plusieurs fois différents programmes sur différents fichiers pendant
quelques secondes. À l\textquoteright issue de cette étape, il y a
plusieurs fichiers contenant les informations sur les temps d'exécutions
des graphes audio dans un répertoire. Le script va alors parcourir
ces fichiers pour générer des graphiques présentant des statistiques
sur les exécutions.

\part*{Comparaison des Algorithmes d'ordonnancement}

Dans cette partie, nous allons réaliser une comparaison des performances
des différents ordonnancements.

\section*{Les caractéristiques des systèmes exécutant ces graphes}

L\textquoteright architecture des machines, qui vont exécuter ces
DAG, considérées dans le cadre de ce projet correspond aux machines
utilisées dans la vie courante (i.e. ordinateurs de bureau et portables).
Ce sont donc des systèmes à mémoire partagée dotés d\textquoteright un
nombre plutôt restreint de processeurs. Ce sont des systèmes homogènes
car les processeurs ont tous la même vitesse.

De plus, les systèmes sur lesquels fonctionnent la plupart des SIM,
ne sont pas des systèmes temps réel (les systèmes de la famille Unix
(GNU/Linux ou MacOS) ou Windows, par exemple). Même si les noyaux
de ces systèmes peuvent avoir une notion de processus avec une priorité
temps réel, ils ne sont pas préemptifs et ne peuvent donc pas toujours
garantir le temps de réponse à une interruption, ni avoir certaines
autres garanties temps réel.

~

Ne pouvant pas avoir un contrôle total sur l\textquoteright ordonnancement
des threads/processus constituant les SIM (parfois, il n\textquoteright est
même pas possible d\textquoteright avoir une priorité temps réel),
il se peut que parfois le SIM ne puisse pas avoir assez de temps processeur
à un certain moment pour respecter la contrainte temps réel. Il peut
aussi y avoir une perte de performance par rapport à la gestion des
caches des processeurs : par exemple, s\textquoteright il y a une
tâche B dépendant du résultat d\textquoteright une tâche A, si A s\textquoteright exécute
sur un processeur et que B s\textquoteright exécute ensuite sur un
processeur différent, le résultat de A sera obligé de transiter par
la mémoire centrale de la machine pour pouvoir être lue par B, ce
qui prendrait plus de temps que si B pouvait ensuite s\textquoteright exécuter
sur le même processeur que A et que le résultat était resté dans le
cache; de manière similaire, si A et B vont s\textquoteright exécuter
sur un même processeur, s\textquoteright il y a un autre processus
(ou même un autre thread du SIM) qui s\textquoteright exécute entre
les deux, sur ce processeur, alors ce dernier peut provoquer l'éviction
du résultat de A depuis le cache du processeur, avant que B n\textquoteright ait
pu s\textquoteright en servir. Nous serons donc dans une démarche
de \textquotedblleft best-effort\textquotedblright{} en essayant de
trouver les meilleures solutions vis-à-vis des limitations que ces
systèmes nous imposent.

~

Dans cette optique regarder juste le temps moyen uniquement n'est
pas pertinent, il faut aussi regarder les pires cas et le nombre de
deadlines dépassées, ce que nous recherchons c'est des algorithmes
qui sont statistiquement stables et qui dépassent le moins possible
le temps qui leurs est impartis, ainsi nous tendrons a privilégier
un algorithme qui est moins bon en moyenne mais plus stable dans les
résultats.

La machine utilisée pour faire les mesures est un Ubuntu 18.04 avec
le noyau linux : 4.15.0-50-generic, un processeur Intel i5-7200U @2.50GHz
avec 3072 KB de cache et 4 c½urs, avec le gouverneur en mode performances.

Les paramètres qui peuvent influencer les résultat de l\textquoteright exécution
sur un même dag sont:
\begin{itemize}
\item Le nombre de threads utilisées
\item La taille des buffers, qui fait varier le temps d'un cycle ainsi le
délais. Les valeurs courantes que nous étudierons sont : 1024, 512,
256, 128, 64, 32
\item L'activité de la machine, les résultats ne serons pas le même avec
une machine au repos, et une autre qui a une charge de travail importante
comme de la compilation par exemple.
\end{itemize}

\subsubsection*{Les Configurations de DAGS étudiées}
\noindent \begin{flushleft}
Nous allons concentrer notre analyse sur trois configurations particulières
de DAGS :
\par\end{flushleft}
\begin{itemize}
\item \begin{flushleft}
La ligne; chaîne de tâches ou chaqu\textquoteright une s'exécute l\textquoteright une
après l\textquoteright autre, dans cette configuration la parallélisation
est impossible, toute exécution est nécessairement séquentielle, ces
DAGS vont nous servir de \textquotedblleft groupe témoin\textquotedblright{}
afin de comparer le surcoût de chaque méthode. 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.25]{dot/ligne5/ligne5-1}
\par\end{centering}
\caption{Exemples de Ligne avec 5 n½uds}

\end{figure}
\par\end{flushleft}
\item \begin{flushleft}
Le Râteau; dans cette configuration c\textquoteright est plusieurs
lignes qui s\textquoteright unissent au bout, cette forme est celle
ou la parallélisation doit être la plus efficace, et donner les meilleurs
gains. 
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.25]{dot/rateau11/rateau11-1}
\par\end{centering}
\caption{Exemples de Râteau avec 11 n½uds}
\end{figure}
\par\end{flushleft}
\item \begin{flushleft}
Finalement le losange qui se trouve entre les deux, il s'agit d'un
DAG qui commence linéairement puis qui va se paralléliser petit a
petit pour enfin se ré-linéariser. On s'attend que l\textquoteright exécution
séquentielle soit plus efficace sur les petits losanges, mais que
le gain de la parallélisation soit significatif sur les plus grands.
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.25]{dot/losange16/losange16-1}
\par\end{centering}
\caption{Exemples de Losange avec 16 n½uds}
\end{figure}
\par\end{flushleft}

\end{itemize}
{[}Expliquer les mesures faites{]}

\subsection*{Première variable: le nombre de threads}

Pour cette première série de mesures nous allons fixer la taille du
buffer a \textbf{128}, et réaliser les mesures sur une machine au
repos.

Nous commençons avec la ligne, pour pouvoir comparer le surcoût de
chaque méthode {[}Figure 5 \vpageref{fig 5} {]}.

\noindent 
\begin{figure}[H]
\begin{raggedright}
\label{fig 5}\subfloat[Ligne avec \textbf{2} threads]{\noindent \begin{raggedright}
\includegraphics[scale=0.4]{ligne_2_128/average}\includegraphics[scale=0.4]{ligne_2_128/misses}\includegraphics[scale=0.4]{ligne_2_128/worst}
\par\end{raggedright}
}
\par\end{raggedright}
\subfloat[Ligne avec \textbf{3} threads]{\noindent \begin{raggedright}
\includegraphics[scale=0.4]{ligne_3_128/average}\includegraphics[scale=0.4]{ligne_3_128/misses}\includegraphics[scale=0.4]{ligne_3_128/worst}
\par\end{raggedright}
}

\noindent \subfloat[Ligne avec \textbf{4} threads]{\includegraphics[scale=0.4]{ligne_4_128/average}\includegraphics[scale=0.4]{ligne_4_128/misses}\includegraphics[scale=0.4]{ligne_4_128/worst}

}\caption{Ligne avec threads variable}
\end{figure}

\noindent Puis avec le Losange {[}Figure 6 \vpageref{fig 6} {]}.
\begin{figure}[H]
\begin{raggedright}
\label{fig 6}\subfloat[Losange avec \textbf{2} threads]{\noindent \begin{raggedright}
\includegraphics[scale=0.4]{losange_2_128/average}\includegraphics[scale=0.4]{losange_2_128/misses}\includegraphics[scale=0.4]{losange_2_128/worst}
\par\end{raggedright}
}
\par\end{raggedright}
\subfloat[Losange avec \textbf{3} threads]{\noindent \begin{raggedright}
\includegraphics[scale=0.4]{losange_3_128/average}\includegraphics[scale=0.4]{losange_3_128/misses}\includegraphics[scale=0.4]{losange_3_128/worst}
\par\end{raggedright}
}

\noindent \subfloat[Losange avec \textbf{4} threads]{\noindent \begin{raggedright}
\includegraphics[scale=0.4]{losange_4_128/average}\includegraphics[scale=0.4]{losange_4_128/misses}\includegraphics[scale=0.4]{losange_4_128/worst}
\par\end{raggedright}
}\caption{Losange avec threads variable}
\end{figure}
Et avec le Râteau {[}Figure 7 \vpageref{Figure  7} {]}.

\noindent 
\begin{figure}[H]
\begin{raggedright}
\label{Figure  7}\subfloat[Rateau avec \textbf{2} threads]{\noindent \begin{raggedright}
\includegraphics[scale=0.4]{rateau_2_128/average}\includegraphics[scale=0.4]{rateau_2_128/misses}\includegraphics[scale=0.4]{rateau_2_128/worst}
\par\end{raggedright}
}
\par\end{raggedright}
\subfloat[Râteau avec \textbf{3} threads]{\noindent \begin{raggedright}
\includegraphics[scale=0.4]{rateau_3_128/average}\includegraphics[scale=0.4]{rateau_3_128/misses}\includegraphics[scale=0.4]{rateau_3_128/worst}
\par\end{raggedright}
}

\noindent \subfloat[Râteau avec \textbf{4} threads]{\noindent \begin{raggedright}
\includegraphics[scale=0.4]{rateau_4_128/average}\includegraphics[scale=0.4]{rateau_4_128/misses}\includegraphics[scale=0.4]{rateau_4_128/worst}
\par\end{raggedright}
}\caption{Râteau avec threads variable}
\end{figure}

Dans un Premier temps on va s\textquoteright intéresser aux dépassements
de délais, on peu constater dans un premier temps que le nombre de
threads optimal sur cette machine est de 3 et cela quelque soit la
configuration étudiée. Cela peut peut sembler contre-intuitif compte
tenu qu'il y a 4 c½urs, on peut que la présence du thread de mesures
impacte les performances de plus car il interrompt les threads du
pool, on peut émettre l\textquoteright hypothèse que le cache lui
étant partagé, il est un frein a l'utilisation des 4 c½urs de la machine.

Secondement en observant les temps moyens et les pires temps pour
la ligne, on peut constater que, hormis des irrégularités des méthodes
HLFET et EFT, les différentes méthodes sont très proches, avec un
léger surcoût de la parallélisation explicable par l'overhead nécessaire
a la gestions des pool de threads et des locks.

Pour le Losange on note l'irrégularité de HLFET qui détonne par rapport
aux trois autres méthodes.

\subsection*{Seconde variable: la taille du buffer}

Plus la taille du buffer est petite moins le système a de latence
mais plus les deadlines sont serrées et plus la machine es mise a
contribution. On a deux configurations : un petit buffer entre 16
et 256, quand on cherche a avoir de la réactivité, par exemple pour
des musiciens qui souhaitent avoir un retour son, pour de la retransmission
en direct, et a l'opposé un plus gros buffer a partir de 512 quand
on peut se permettre de ménager sa machine.

On va fixer le nombre de threads et faire varier la taille du buffer
de 16, 32, 64,128, 256, et enfin 512.

\begin{figure}[H]
\subfloat[buffer de 16]{\includegraphics[scale=0.4]{ligne_3_16/misses}}\subfloat[buffer de 32]{\includegraphics[scale=0.4]{ligne_3_32/misses}}\subfloat[buffer de 64]{\includegraphics[scale=0.4]{ligne_3_64/misses}}
\end{figure}
\begin{figure}[H]
\subfloat[buffer de 128]{\includegraphics[scale=0.4]{ligne_3_128/misses}}\subfloat[buffer de 256]{\includegraphics[scale=0.4]{ligne_3_256/misses}}\subfloat[buffer de 512]{\includegraphics[scale=0.4]{ligne_3_512/misses}}

\caption{variations pour la ligne}
\end{figure}

\begin{figure}[H]
\subfloat[buffer de 16]{\includegraphics[scale=0.4]{\string"losange_3_16 /misses\string".eps}}\subfloat[buffer de 32]{\includegraphics[scale=0.4]{\string"losange_3_32 /misses\string".eps}}\subfloat[buffer de 64]{\includegraphics[scale=0.4]{\string"losange_3_64 /misses\string".eps}}
\end{figure}
\begin{figure}[H]
\subfloat[buffer de 128]{\includegraphics[scale=0.4]{losange_3_128/misses}}\subfloat[buffer de 256]{\includegraphics[scale=0.4]{\string"losange_3_256 /misses\string".eps}}\subfloat[buffer de 512]{\includegraphics[scale=0.4]{\string"losange_3_512 /misses\string".eps}}

\caption{variations pour le losange}
\end{figure}

\begin{figure}[H]
\subfloat[buffer de 16]{\includegraphics[scale=0.4]{\string"rateau_3_16 /misses\string".eps}}\subfloat[buffer de 32]{\includegraphics[scale=0.4]{\string"rateau_3_32 /misses\string".eps}}\subfloat[buffer de 64]{\includegraphics[scale=0.4]{\string"losange_3_64 /misses\string".eps}}
\end{figure}
\begin{figure}[H]
\subfloat[buffer de 128]{\includegraphics[scale=0.4]{rateau_3_128/misses}}\subfloat[buffer de 256]{\includegraphics[scale=0.4]{\string"rateau_3_256 /misses\string".eps}}\subfloat[buffer de 512]{\includegraphics[scale=0.4]{\string"rateau_3_512 /misses\string".eps}}

\caption{variations pour le râteau}
\end{figure}


\subsection*{Dernière variable: la charge de la machine}

\begin{figure}[H]
\includegraphics[scale=0.4]{ligne_3_128/average}\includegraphics[scale=0.4]{ligne_3_128/misses}\includegraphics[scale=0.4]{ligne_3_128/worst}

\caption{ligne au repos}
\end{figure}

\begin{figure}[H]
\includegraphics[scale=0.4]{\string"ligne_3_128 _charge/average\string".eps}\includegraphics[scale=0.4]{\string"ligne_3_128 _charge/misses\string".eps}\includegraphics[scale=0.4]{\string"ligne_3_128 _charge/worst\string".eps}

\caption{ligne en regardant un film}
\end{figure}

\begin{figure}[H]
\includegraphics[scale=0.4]{losange_3_128/average}\includegraphics[scale=0.4]{losange_3_128/misses}\includegraphics[scale=0.4]{losange_3_128/worst}

\caption{losange au repos}
\end{figure}

\begin{figure}[H]
\includegraphics[scale=0.4]{losange_3_128_charge/average}\includegraphics[scale=0.4]{losange_3_128_charge/misses}\includegraphics[scale=0.4]{losange_3_128_charge/worst}

\caption{losange en regardant un film}
\end{figure}

\begin{figure}[H]
\includegraphics[scale=0.4]{rateau_3_128/average}\includegraphics[scale=0.4]{rateau_3_128/misses}\includegraphics[scale=0.4]{rateau_3_128/worst}

\caption{râteau au repos}
\end{figure}

\begin{figure}[H]
\includegraphics[scale=0.4]{rateau_3_128_charge/average}\includegraphics[scale=0.4]{rateau_3_128_charge/misses}\includegraphics[scale=0.4]{rateau_3_128_charge/worst}

\caption{râteau en regardant un film}
\end{figure}


\subsection*{La stabilité des Algorithmes}

Pour comparer la stabilité des Algorithmes on va faire des histogramme
avec 3 threads et des buffers de 2048, a fin d'avoir des cycles plus
longs et de mieux voir les variations.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{hist/hist_all_ligne}
\par\end{centering}
\caption{Histogramme pour une Ligne a 100 n½uds}

\begin{centering}
\includegraphics[scale=0.5]{hist/hist_all_losange}
\par\end{centering}
\caption{Histogramme pour un losange a 156 n½uds}

\begin{centering}
\includegraphics[scale=0.5]{hist/hist_all_rateau}
\par\end{centering}
\caption{Histogramme pour un Râteau a 101 n½uds}
\end{figure}

On peut dans un premier temps constater que sur l'histogramme de la
ligne que HLFET a un overhead significatif sur les autres méthodes.
Les stabilités sont proches pour les quatre méthodes.

Sur l'histogramme du losange on voit une mauvaise stabilité du vol
de tâches, un peu médiocre de la part de HLFET. Les plus stables sont
le calcul séquenciel en premier puis en second ETF qui est cependant
notablement plus rapide.

Pour le Rateau en revanche on voir que l\textquoteright exécution
séquentielle n'est plus aussi stable, les méthodes parallélises gagnent
en stabilité avec ETF et le vol de tache qui sont notablement plus
stables.

\subsection*{Conclusions}

On peut dans un premier temps noter la fiabilité et la constance de
l\textquoteright exécution séquentielle, qui dans les différents cas
de figure est de loin la méthode la plus fiable et la plus efficace.
Pour les autre méthodes on voit qu'HLFET est vraiment a la traîne
au niveau des performances. L'algorithme ETF offre une stabilité,
visible nottement

\part*{Évolutions possibles}

{[}Idée général de la partie : Critique de notre implémentation, ce
qui pourrait être fait pour améliorer les résultats. Bien pour faire
une conclusion \textquotedblleft ouverte\textquotedblright{} du rapport{]}
Pistes pour améliorer les performances de l\textquoteright implémentation
{[}Le code n\textquoteright a pas été très bien optimisé en raison
du temps que cela demandait{]} {[}Réduction de la quantité de locks
utilisés. Les placer seulement aux endroits nécessaires et cloner
le reste des données pour chaque thread{]} {[}Encore plus loin, remplacer
les lock par des \textquotedblleft traits unsafe\textquotedblright{}
quand c\textquoteright est possible (voir https://doc.rust-lang.org/nomicon/concurrency.html
){]} {[}Les threads n\textquoteright ont pas une priorité temps réel,
on pourrait se servir de https://github.com/padenot/audio\_thread\_priority
?{]} Fonctionnalité(s) nouvelle(s) {[}Utiliser MPI pour CPFD ?{]}
{[}Il y a des choses meilleures que MPI ?{]}
\begin{thebibliography}{10}
\bibitem{key-1}: Yann Orlarey, Stéphane Letz, Dominique Fober, Work
stealing scheduler for automatic parallelization in faust, LAC 2010

\bibitem{key-2}: MA Kiefer, K Molitorisz, J Bieler, Parallelizing
a Real-Time Audio Application---A Case Study in Multithreaded Software
Engineering, Parallel and Distributed Processing Symposium Workshop
(IPDPSW), 2015

\bibitem{key-3}: YK Kwok, I Ahmad, Static scheduling algorithms for
allocating directed task graphs to multiprocessors, ACM Computing
Surveys

\bibitem{key-4} \textbf{https://www.graphviz.org/} , derniere visite
le 24/05/2019

\bibitem{key-5}\textbf{https://www.rust-lang.org/} , derniere visite
le 24/05/2019

\bibitem{key-6}\textbf{https://llvm.org/} , derniere visite le 24/05/2019

\bibitem{key-7}\textbf{http://www.jackaudio.org/} , derniere visite
le 24/05/2019

\bibitem{key-12}\textbf{https://docs.rs/crossbeam/0.7.1/crossbeam/}
, derniere visite le 24/05/2019

\bibitem{key-13}\textbf{https://docs.rs/core\_affinity/0.5.9/core\_affinity/}
, derniere visite le 24/05/2019

\bibitem{key-14}\textbf{https://criterion.readthedocs.io/en/master/}
, derniere visite le 24/05/2019

\bibitem{key-15}\textbf{https://pest.rs/} , derniere visite le 24/05/2019
\end{thebibliography}

\end{document}
